# ğŸ©º Blood Pressure & Stroke Prediction System

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du **Master USPN (Big Data)**. Il s'agit d'un Ã©cosystÃ¨me complet de traitement de donnÃ©es de santÃ© en temps rÃ©el utilisant une architecture distribuÃ©e pour prÃ©dire les risques d'AVC (Accident Vasculaire CÃ©rÃ©bral).
---

## ğŸš€ DÃ©ploiement (Installation & Run)

Suivez ces Ã©tapes pour installer l'environnement et lancer le pipeline de prÃ©diction sur votre machine locale.

### 1. PrÃ©requis
* **Docker Desktop** : Assurez-vous qu'il est installÃ© et que le moteur WSL2 est activÃ© (pour Windows).
* **Python 3.10+** : VÃ©rifiez votre version avec `python --version`.
* **Git** : Pour cloner le rÃ©pertoire.

### 2. RÃ©cupÃ©ration du Projet
Ouvrez un terminal (PowerShell ou Bash) et exÃ©cutez :
```bash
git clone https://github.com/Ange-Paul-Emmanuel/blood_pressure_obersavability_docker_project/tree/master
cd "projet blood pressure"
```
### 3. Lancement de l'Infrastructure (Docker)
DÃ©marrez tous les services (Zookeeper, Kafka, Elasticsearch, Kibana, Kafka-UI) en arriÃ¨re-plan

```bash
docker-compose up -d
```
- VÃ©rification : Attendez environ 30 secondes, puis tapez docker ps pour vÃ©rifier que tous les containers affichent le statut Up

### 4. Configuration de l'environnement Python
```bash
# CrÃ©ation de l'environnement
python -m venv venv

# Activation (Windows)
.\venv\Scripts\activate

# Activation (Mac/Linux)
# source venv/bin/activate

# Installation des bibliothÃ¨ques nÃ©cessaires
pip install -r requirements.txt
```

### 5. ExÃ©cution du Pipeline (Ordre de lancement)

Pour que le flux de donnÃ©es soit correctement traitÃ©, ouvrez quatre terminaux diffÃ©rents et lancez les scripts dans l'ordre suivant :
```bash
# ExcÃ©cute le Simulateur de donnÃ©es mÃ©dicales
generator.py

# DÃ©marre l'envoi des donnÃ©es patients simulÃ©es vers Kafka.
python producer.py

# Lance l'IA qui Ã©coute Kafka et prÃ©dit les risques d'AVC.
python medical_agent.py

# PrÃ©pare la rÃ©ception et l'indexation dans Elasticsearch.
python consumer.py
```
### 6. AccÃ¨s aux Interfaces Graphiques
Une fois le pipeline en marche, vous pouvez surveiller le systÃ¨me via :

Kafka UI : http://localhost:8080 (pour voir les messages dans les topics).

Kibana : http://localhost:5601 (pour visualiser les dashboards et les alertes).


---

## ğŸ“‚ Arborescence du Projet (Project Tree)

```text
PROJET BLOOD PRESSURE/
â”œâ”€â”€ outputs_bp/                 # Artefacts et modÃ¨les exportÃ©s
â”‚   â”œâ”€â”€ dataset_enrichi.csv        # Dataset aprÃ¨s feature engineering
â”‚   â””â”€â”€ my_random_forest_avc.joblib # ModÃ¨le Random Forest entraÃ®nÃ©
â”œâ”€â”€ venv/                       # Environnement virtuel Python
â”œâ”€â”€ .env                        # Configuration des accÃ¨s (Clef API)
â”œâ”€â”€ .gitignore                  # Fichiers Ã  exclure du versioning
â”œâ”€â”€ docker-compose.yml          # Orchestration des conteneurs Docker(Kafka, Zookeeper, ES, Kibana)
â”œâ”€â”€ producer.py                 # Ingestion des donnÃ©es patients (Source)
â”œâ”€â”€ medical_agent.py            # Agent IA (PrÃ©diction AVC en temps rÃ©el)
â”œâ”€â”€ consumer.py                 # Indexation finale dans Elasticsearch
â”œâ”€â”€ generator.py                # Simulateur de donnÃ©es mÃ©dicales
â”œâ”€â”€ reset_topic.py              # Script utilitaire de purge Kafka
â”œâ”€â”€ requirements.txt            # DÃ©pendances du projet
â”œâ”€â”€ ML_Model.ipynb              # Notebook d'entraÃ®nement et modÃ¨le de prÃ©diction
â””â”€â”€ patients_sains.json         # Stock localement les donnÃ©es des patients sains
